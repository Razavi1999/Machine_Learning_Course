\begin{boxC}
    الگوریتم‌های 
    \lr{WordPiece(BERT)}
    و
    \lr{BPE(GPT)}
    از معروف‌ترین الگوریتم‌ها برای نشانه‌گذاری کلمات هستند.
    این الگوریتم‌ها متن را به 
    \textbf{زیرکلمه‌ها}
    نشانه‌گذاری می‌کنند.

    در الگوریتم 
    \lr{WordPiece}
     با رویکرد ادغام حریصانه ، تلاش می‌کند تا 
     \textbf{دنباله‌ای از کاراکترها}
     با بیشترین تکرار برای تشکیل واحدهای زیرکلمه ساخته‌شوند.

    رویکرد این الگوریتم بر آن است که با توجه به واژگان احتمال داده‌های آموزشی به حداکثر خود برسد.

    هنگام توکن کردن یک کلمه، الگوریتم ابتدا سعی می کند آن را به زیرکلمه های کامل از واژگان تبدیل کند. اگر کلمه ای را نتوان به طور کامل نشانه گذاری کرد، آن را به واحدهای فرعی کوچکتر تقسیم می کند. این به الگوریتم اجازه می دهد تا کلمات خارج از واژگان را با تجزیه آنها به واحدهای فرعی شناخته شده مدیریت کند.

یکی از مزیت‌های الگوریتم این است که می‌تواند کلمات نادر یا دیده نشده را با تجزیه آنها به واحدهای زیرکلمه که بخشی از واژگان هستند، مدیریت کند.
این به بهبود تعمیم مدل های آموزش داده شده بر روی داده های متنی کمک می کند.
    
\end{boxC}


\begin{boxC}
    الگوریتم
    \lr{Byte Pair Encoding(BPE)} : 
    با
\textbf{کاراکترهای منفرد }
    به عنوان واژگان اولیه شروع می شود و به طور مکرر متداول ترین جفت نمادها را برای ایجاد واحدهای بزرگ‌تر جدید مانند زیرکلمه ادغام می کند.

    به همین علت این الگوریتم انعطاف‌پذیری بیشتری در ساختن کلمات و همچنین تنوع بیشتری در ساختن کلمات جدید دارد.

    همچنین فرآیند ادغام کاراکترها تا رسیدن به یک معیارهمگرایی ادامه خواهد داشت . یعنی یک معیارهمگرایی ازپیش‌ تعریف‌شده برای الگوریتم وجود دارد.
\end{boxC}

\begin{boxC}
    \begin{itemize}
        \item رویکرد 
        \lr{WordPiece}
        از کاراکتر تنها شروع به ساختن زیرکلمه می‌کند اما رویکرد
        \lr{BPE}
        از مجموعه کاراکتر متوالی برای شروع ساختن زیرکلمه استفاده می‌کند.

        \item 
        استراتژی رویکرد 
        \lr{WordPiece}
        بر مبنای
        \lr{LikeLihood-based merging}
        می‌باشد.
        اما استراتژی رویکرد 
        \lr{BPE}
        بر مبنای میزان تکرار و وقوع کاراکترها می‌باشد.
    
    
        \item 
        در رویکرد
        \lr{BPE}
        ما میزانی برای میزان همگرایی داریم که ازپیش تعریف شده است بر اساس تعداد کلمات \lr{vocabulary}
        اما در رویکرد
        \lr{WordPiece}
        بر اساس 
        \lr{likelihood}
        به ساختن زیرکلمات ادامه خواهیم‌داد.
    \end{itemize}
    
\end{boxC}

\begin{boxC}
    در صورت استفاده از پکیج‌های پیش‌ساخته پایتون تعداد توکن‌های هر دوی این رویکردهای نشانه‌گذاری ۳۰۰۰۰ خواهد بود.
\end{boxC}

\begin{boxC}
    دلایل استفاده از این دو 

    برای مدل‌های زبانی بزرگ به صورت زیر خواهند بود :
    \begin{itemize}
        \item 
        سازگاری با مدل‌های از پیش آموزش دیده مانند 
        \lr{BERT , GPT}
        
        \item 
        مدیریت کارآمد توکن‌ها
        
        \item 
        توکن‌کردن زیرکلمات

        \item 
        مدیریت کلمات خارج از مجموعه لغات
    \end{itemize}
\end{boxC}
